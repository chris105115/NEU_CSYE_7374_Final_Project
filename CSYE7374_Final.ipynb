{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: All necessary imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "import random\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import urllib.request\n",
    "import pathlib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset from https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset - Chest X-rays (IU)\n",
    "# Download latest version\n",
    "\n",
    "\n",
    "# %pip install --upgrade kagglehub\n",
    "\n",
    "# # Get current working directory\n",
    "# parent_dir = pathlib.Path(os.getcwd())\n",
    "\n",
    "# # Define the path to the data directory\n",
    "# data_dir = parent_dir / \"Data\"\n",
    "\n",
    "# # Create the data directory if it doesn't exist\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# # Download the dataset from Kaggle\n",
    "# # If you cannot download the dataset from Kaggle, you can go and check this website:https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university\n",
    "# path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\", path=data_dir)\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_xray_images(image_dir, excel_file, output_dir):\n",
    "    \"\"\"\n",
    "    Based on the projection information provided in the excel file,\n",
    "    classify the X-ray images into \"Frontal\" and \"Lateral\" directories.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): the location of the Chest X-ray images.\n",
    "        excel_file (str): the csv file containing projection information.\n",
    "        output_dir (str): output directory to save Classified Images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read Excel file (\"indiana_projections\")\n",
    "    df = pd.read_csv(excel_file)\n",
    "\n",
    "    # Classify images based on projection information that provided in the excel file\n",
    "    # Create output directories for Frontal and Lateral images\n",
    "    frontal_dir = os.path.join(output_dir, \"Frontal\")\n",
    "    lateral_dir = os.path.join(output_dir, \"Lateral\")\n",
    "    os.makedirs(frontal_dir, exist_ok=True)\n",
    "    os.makedirs(lateral_dir, exist_ok=True)\n",
    "\n",
    "    # Traver every row in the Excel file\n",
    "    for index, row in df.iterrows():\n",
    "        # Store the image name under the \"filename\" column\n",
    "        image_name = row[\"filename\"]  \n",
    "        # Store the projection information under the \"projection\" column\n",
    "        projection = row[\"projection\"]\n",
    "\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "\n",
    "        # According to the projection information, copy the image to the corresponding directory\n",
    "        if projection == \"Frontal\":\n",
    "            shutil.copy(image_path, os.path.join(frontal_dir, image_name))\n",
    "        elif projection == \"Lateral\":\n",
    "            shutil.copy(image_path, os.path.join(lateral_dir, image_name))\n",
    "\n",
    "# Get current working directory\n",
    "parent_dir = pathlib.Path(os.getcwd())\n",
    "\n",
    "# Define the path to the images and csv file\n",
    "image_directory = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"images\"  \n",
    "excel_filepath = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"indiana_projections.csv\"\n",
    "output_directory = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"Classified_Images\"\n",
    "\n",
    "classify_xray_images(image_directory, excel_filepath, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Descriptions and Uid from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions: 3851\n"
     ]
    }
   ],
   "source": [
    "def description_of_images(excel_file):\n",
    "    \"\"\"\n",
    "    This function provides a description of the images in the dataset.\n",
    "\n",
    "    Args:\n",
    "        excel_file (str): the csv file containing information about images.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains (uid, description).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # A tuple list to store the (uid, description) pairs\n",
    "    description_tuples = []\n",
    "    # A tuple list to store the (uid, description) pairs\n",
    "\n",
    "\n",
    "    # Read Excel file (\"indiana_projections\")\n",
    "    df = pd.read_csv(excel_file)\n",
    "\n",
    "    def create_description_tuple(row):\n",
    "        \"\"\"According to csv file build up (uid, description) tuplesã€‚\"\"\"\n",
    "        uid = row['uid'] \n",
    "        description_parts = []\n",
    "\n",
    "        if pd.notna(row['MeSH']):\n",
    "            description_parts.append(f\"MeSH: {row['MeSH']}.\")\n",
    "\n",
    "        if pd.notna(row['Problems']):\n",
    "            description_parts.append(f\"Problems: {row['Problems']}.\")\n",
    "\n",
    "        if pd.notna(row['image']):\n",
    "            description_parts.append(f\"Image: {row['image']}.\")\n",
    "\n",
    "        if pd.notna(row['indication']):\n",
    "            description_parts.append(f\"Indication: {row['indication']}.\")\n",
    "\n",
    "        if pd.notna(row['comparison']):\n",
    "            description_parts.append(f\"comparison: {row['comparison']}.\")\n",
    "\n",
    "        if pd.notna(row['findings']):\n",
    "            description_parts.append(f\"Findings: {row['findings']}.\")\n",
    "\n",
    "        if pd.notna(row['impression']):\n",
    "            description_parts.append(f\"Impression: {row['impression']}.\")\n",
    "\n",
    "        description = \" \".join(description_parts)  \n",
    "        return (uid, description)\n",
    "\n",
    "    description_tuples = df.apply(create_description_tuple, axis=1).tolist()\n",
    "    print(f\"Total number of descriptions: {len(description_tuples)}\")\n",
    "\n",
    "    # print(description_tuples)\n",
    "    return description_tuples\n",
    "\n",
    "    \n",
    "excel_filepath_2 = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"indiana_reports.csv\"\n",
    "description_data = description_of_images(excel_filepath_2)\n",
    "\n",
    "# Convert the list of tuples to a DataFrame\n",
    "df_descriptions = pd.DataFrame(description_data, columns=['uid', 'description'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = os.path.join(parent_dir / \"Data\" / \"chest-xrays-indiana-university\", \"image_descriptions.csv\")\n",
    "df_descriptions.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3818 Frontal images and 3648 Lateral images\n",
      "Found 3510 paired Frontal/Lateral datasets with descriptions\n",
      "The length of dataset (list of dictionaries): 3510\n",
      "Sample:\n",
      "Frontal Tensor: torch.Size([1, 128, 128])\n",
      "Lateral Tensor: torch.Size([1, 128, 128])\n",
      "Description Tensor: torch.Size([1, 128, 128])\n",
      "tensor([[[ 2.0447e-01,  6.8433e-02, -4.8567e-02,  ..., -3.3070e-02,\n",
      "          -1.2024e-01,  1.1317e-01],\n",
      "         [-2.8492e-01,  2.9900e-01,  3.9792e-01,  ..., -4.6434e-01,\n",
      "          -2.8159e-01,  1.1979e-02],\n",
      "         [-3.1300e-01,  3.1530e-01,  3.3854e-01,  ..., -4.7112e-01,\n",
      "          -2.6609e-01,  3.0567e-04],\n",
      "         ...,\n",
      "         [ 8.3440e-02,  1.9712e-01,  7.8786e-02,  ..., -2.0232e-01,\n",
      "          -3.9442e-01, -1.2849e-02],\n",
      "         [-3.7950e-01,  9.9198e-02,  1.3823e-01,  ..., -1.5076e-01,\n",
      "          -3.6741e-01,  2.0104e-01],\n",
      "         [-4.0654e-01,  9.0731e-02,  1.6034e-01,  ..., -9.8525e-02,\n",
      "          -4.2525e-01,  2.6769e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Dataset class for loading paired/unpaired Frontal-Lateral X-ray data (PNG).\"\"\"\n",
    "\n",
    "    def __init__(self, frontal_dir, lateral_dir, excel_file, paired=True, transform=None, cache_size=0, max_length=128, embedding_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frontal_dir (str): Directory containing Frontal X-ray images.\n",
    "            lateral_dir (str): Directory containing Lateral X-ray images.\n",
    "            excel_file(str): csv file containing image descriptions.\n",
    "            paired (bool): If True, uses paired Frontal-Lateral data, else random unpaired selection.\n",
    "            transform: Optional transforms to apply to images.\n",
    "            cache_size (int): Number of images to cache in memory (0 for no caching).\n",
    "            max_length (int): max length of the description.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        # Add resize transform\n",
    "        self.resize_transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Store initialization parameters\n",
    "        self.frontal_dir = frontal_dir\n",
    "        self.lateral_dir = lateral_dir\n",
    "        self.excel_file = excel_file\n",
    "        self.transform = transform\n",
    "        self.paired = paired\n",
    "        self.cache_size = cache_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_reducer = nn.Linear(312, embedding_dim)\n",
    "\n",
    "        # Get lists of all PNG files\n",
    "        self.frontal_files = sorted([f for f in os.listdir(frontal_dir) if f.endswith('.png')])\n",
    "        self.lateral_files = sorted([f for f in os.listdir(lateral_dir) if f.endswith('.png')])\n",
    "        print(f\"Found {len(self.frontal_files)} Frontal images and {len(self.lateral_files)} Lateral images\")\n",
    "\n",
    "\n",
    "        # Load descriptions from CSV and create a UID-to-description mapping\n",
    "        self.uid_to_description = self.load_uid_descriptions(excel_file)\n",
    "\n",
    "        # For paired training, find matching Frontal-Lateral pairs based on filename prefix\n",
    "        if self.paired:\n",
    "            self.paired_files = []\n",
    "            for frontal_f in self.frontal_files:\n",
    "                prefix = frontal_f.split('_')[0]  # Extract the prefix (e.g., \"1\")\n",
    "                matching_lateral = [lateral_f for lateral_f in self.lateral_files if lateral_f.split('_')[0] == prefix]\n",
    "                if matching_lateral:\n",
    "                    description = self.uid_to_description.get(prefix, None)\n",
    "                    if description:\n",
    "                        self.paired_files.append((frontal_f, matching_lateral[0], description))\n",
    "                    else:\n",
    "                        print(f\"Warning: Description not found for UID {prefix}\")\n",
    "            print(f\"Found {len(self.paired_files)} paired Frontal/Lateral datasets with descriptions\")\n",
    "            self.data_files = self.paired_files\n",
    "\n",
    "\n",
    "            # -------------------------------- Check the paired files list -------------------------------- #\n",
    "            # Check the paired files, and check if the description is matched with the images\n",
    "            # print(self.paired_files[0])\n",
    "            # -------------------------------- Check the paired files list -------------------------------- #\n",
    "\n",
    "\n",
    "        else:\n",
    "            # For unpaired, just use Frontal files and randomly select Lateral later\n",
    "            self.data_files = [(frontal_f, None, None) for frontal_f in self.frontal_files]\n",
    "\n",
    "        # Initialize cache dictionary\n",
    "        self.cache = {}\n",
    "\n",
    "        # Load descriptions from CSV\n",
    "        self.descriptions = self.load_uid_descriptions(excel_file)\n",
    "\n",
    "        # Choose a model name for the tokenizer and text encoder\n",
    "        # model_name = \"openai/clip-vit-base-patch32\"\n",
    "        # model_name = \"bert-base-uncased\" # Embedding Dimension size: 768\n",
    "        # model_name = \"distilbert-base-uncased\" # Embedding Dimension size: 768\n",
    "        model_name = \"huawei-noah/TinyBERT_General_4L_312D\" # Embedding Dimension size: 312\n",
    "\n",
    "        # Load tokenizer and text encoder based on model name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        self.text_encoder = AutoModel.from_pretrained(model_name) \n",
    "\n",
    "    def load_uid_descriptions(self, excel_file):\n",
    "        \"\"\"Loads descriptions from CSV and returns a dictionary of UID to description.\"\"\"\n",
    "        df = pd.read_csv(excel_file)\n",
    "        uid_to_description = dict(zip(df['uid'].astype(str), df['description']))\n",
    "        return uid_to_description\n",
    "\n",
    "    def _load_and_validate_image(self, filename, is_frontal=True):\n",
    "        \"\"\"Load and validate a single PNG image.\"\"\"\n",
    "        dir_path = self.frontal_dir if is_frontal else self.lateral_dir\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "\n",
    "        # Check if image is in cache\n",
    "        if filepath in self.cache:\n",
    "            img = self.cache[filepath]\n",
    "        else:\n",
    "            # Load image using PIL\n",
    "            # Convert PNG file to PIL Image\n",
    "            img = Image.open(filepath).convert('L')  # Convert to grayscale\n",
    "\n",
    "            # Validate image (optional: add more complex validation if needed)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Invalid image: {filename}\")\n",
    "\n",
    "            # Cache image if cache isn't full\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[filepath] = img\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of Frontal-Lateral pairs in the dataset.\"\"\"\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a Frontal-Lateral pair of images.\"\"\"\n",
    "        frontal_file, lateral_file, description = self.data_files[idx]\n",
    "\n",
    "        # Load Frontal image\n",
    "        frontal_img = self._load_and_validate_image(frontal_file, is_frontal=True)\n",
    "\n",
    "        if self.paired:\n",
    "            # Load matching Lateral image for paired data\n",
    "            lateral_img = self._load_and_validate_image(lateral_file, is_frontal=False)\n",
    "        else:\n",
    "            # For unpaired data, randomly select a Lateral image\n",
    "            random_lateral_idx = random.randint(0, len(self.lateral_files) - 1)\n",
    "            lateral_img = self._load_and_validate_image(self.lateral_files[random_lateral_idx], is_frontal=False)\n",
    "\n",
    "        # Convert images to tensors (from PIL to tensor) and apply resize transform\n",
    "        frontal_tensor = self.resize_transform(frontal_img)\n",
    "        lateral_tensor = self.resize_transform(lateral_img)\n",
    "\n",
    "        if self.transform:\n",
    "            frontal_tensor = self.transform(frontal_tensor)\n",
    "            lateral_tensor = self.transform(lateral_tensor)\n",
    "\n",
    "        # Convert description to tensor\n",
    "        inputs = self.tokenizer(description, padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')\n",
    "        description_tensor = self.text_encoder(inputs.input_ids, attention_mask=inputs.attention_mask).last_hidden_state.squeeze(0).unsqueeze(0)\n",
    "        description_tensor = self.embedding_reducer(description_tensor)\n",
    "\n",
    "        # Clean up images if not cached\n",
    "        if frontal_file not in self.cache:\n",
    "            del frontal_img\n",
    "        if lateral_file not in self.cache:\n",
    "            del lateral_img\n",
    "\n",
    "        return {'Frontal': frontal_tensor, 'Lateral': lateral_tensor, 'Description': description_tensor}\n",
    "\n",
    "# Utilize the new csv file with concatenated descriptions and uids\n",
    "excel_file_3 = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"image_descriptions.csv\"\n",
    "frontal_dir = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"Classified_Images\" / \"Frontal\"\n",
    "lateral_dir = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"Classified_Images\" / \"Lateral\"\n",
    "\n",
    "dataset = ChestXrayDataset(\n",
    "    frontal_dir=frontal_dir,\n",
    "    lateral_dir=lateral_dir,\n",
    "    excel_file=excel_file_3,\n",
    "    paired=True\n",
    ")\n",
    "\n",
    "print(f\"The length of dataset (list of dictionaries): {len(dataset)}\")\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"Sample:\")\n",
    "print(\"Frontal Tensor:\", sample['Frontal'].shape)\n",
    "print(\"Lateral Tensor:\", sample['Lateral'].shape)\n",
    "print(\"Description Tensor:\", sample['Description'].shape)\n",
    "print(sample['Description'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (U-net with Time + Cross-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define an Identity module to bypass attention when not used.\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "# Inherit from nn.Module to create a custom TimeEmbedding class.\n",
    "# nn = torch.nn = torch.neuralnetwork\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Projects timesteps into a higher-dimensional space for time conditioning.\"\"\"\n",
    "    # Constructor takes the number of channels as an argument.\n",
    "    def __init__(self, n_channels):\n",
    "        # Call the parent constructor to initialize the module.\n",
    "        # super() is a built-in function that returns a temporary object of the superclass.\n",
    "        super().__init__()\n",
    "        # Define data fields for the number of channels and the time projection layers.\n",
    "        self.n_channels = n_channels\n",
    "        # Define data fields for the time projection layers.\n",
    "        self.time_proj = nn.Sequential(\n",
    "            # Linear layer to project the input time step to the number of channels.\n",
    "            nn.Linear(1, n_channels),\n",
    "            # Activation function to introduce non-linearity.\n",
    "            nn.SiLU(),\n",
    "            # Linear layer to project the output of the activation function back to the number of channels.\n",
    "            nn.Linear(n_channels, n_channels)\n",
    "        )\n",
    "    # Forward propagation method to define the forward pass of the module.\n",
    "    # Takes the input time step as an argument.\n",
    "    def forward(self, t):\n",
    "        # t: [B] -> [B, 1]\n",
    "        # Convert the input time step to a 2D tensor with shape [B, 1] by unsqueezing the last dimension.\n",
    "        t = t.unsqueeze(-1).float()\n",
    "        return self.time_proj(t)  # [B, n_channels]\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A convolutional block with time conditioning and residual connections.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, time_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.time_mlp = nn.Linear(time_channels, out_channels)\n",
    "        self.use_residual = in_channels == out_channels\n",
    "        if not self.use_residual:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "    def forward(self, x, t):\n",
    "        residual = x if self.use_residual else self.residual_conv(x)\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        # Broadcast time embedding over spatial dimensions.\n",
    "        h += self.time_mlp(t)[:, :, None, None]\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        return h + residual\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention module to capture long-range dependencies.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads=4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] -> flatten spatial dims -> [B, HW, C]\n",
    "        size = x.shape[-2:]\n",
    "        x_flat = x.flatten(2).transpose(1, 2)\n",
    "        x_norm = self.ln(x_flat)\n",
    "        attn_out, _ = self.mha(x_norm, x_norm, x_norm)\n",
    "        attn_out = attn_out + x_flat  # Skip connection\n",
    "        ff_out = self.ff_self(attn_out) + attn_out\n",
    "        # Restore spatial dims.\n",
    "        return ff_out.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention module to attend between source and context features.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads=4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_cross = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "    def forward(self, x, context):\n",
    "        # x (query) and context (key/value) both: [B, C, H, W]\n",
    "        size = x.shape[-2:]\n",
    "        x_flat = x.flatten(2).transpose(1, 2)\n",
    "        context_flat = context.flatten(2).transpose(1, 2)\n",
    "        x_norm = self.ln(x_flat)\n",
    "        attn_out, _ = self.mha(x_norm, context_flat, context_flat)\n",
    "        attn_out = attn_out + x_flat  # Skip connection\n",
    "        ff_out = self.ff_cross(attn_out) + attn_out\n",
    "        return ff_out.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net with optional self-attention and cross-attention for diffusion models.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        # Convert time steps information to a higher-dimensional space (time embedding), you can try 256, 512 and so on......\n",
    "        # Dimension of the time embedding vector.\n",
    "        time_channels=256,\n",
    "        n_channels=64,\n",
    "        use_self_attention=True,\n",
    "        use_cross_attention=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            time_channels (int): Dimension of time embedding.\n",
    "            n_channels (int): Base number of channels.\n",
    "            use_self_attention (bool): If True, apply self-attention.\n",
    "            use_cross_attention (bool): If True, apply cross-attention in bottleneck.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        # Context projection (only matters if using cross-attention)\n",
    "        self.context_proj = nn.Conv2d(1, n_channels * 8, kernel_size=1) if use_cross_attention else Identity()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = TimeEmbedding(time_channels)\n",
    "        \n",
    "        # Encoder path\n",
    "        self.inc = ConvBlock(in_channels, n_channels, time_channels)\n",
    "        self.down1 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels, n_channels*2, time_channels),\n",
    "            SelfAttention(n_channels*2) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.down2 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*2, n_channels*4, time_channels),\n",
    "            SelfAttention(n_channels*4) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.down3 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*4, n_channels*8, time_channels),\n",
    "            SelfAttention(n_channels*8) if use_self_attention else Identity()\n",
    "        ])\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        self.bot1 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot_attn = SelfAttention(n_channels*8) if use_self_attention else Identity()\n",
    "        self.cross_attn = CrossAttention(n_channels*8) if use_cross_attention else Identity()\n",
    "        self.bot2 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot3 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        self.up1 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*12, n_channels*4, time_channels),  # Concatenation: x4 (n_channels*8) and x3 (n_channels*4)\n",
    "            SelfAttention(n_channels*4) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.up2 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*6, n_channels*2, time_channels),   # Concatenation: previous output (n_channels*4) with x2 (n_channels*2)\n",
    "            SelfAttention(n_channels*2) if use_self_attention else Identity()\n",
    "        ])\n",
    "        self.up3 = nn.ModuleList([\n",
    "            ConvBlock(n_channels*3, n_channels, time_channels)        # Concatenation: previous output (n_channels*2) with x1 (n_channels)\n",
    "        ])\n",
    "        \n",
    "        # Output convolution\n",
    "        self.outc = nn.Conv2d(n_channels, 1, 1)\n",
    "    \n",
    "    def forward(self, x, t, condition=None, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor [B, C, H, W].\n",
    "            t (torch.Tensor): Timesteps [B].\n",
    "            condition (torch.Tensor, optional): Conditioning image.\n",
    "            context (torch.Tensor, optional): Context image for cross-attention [B, 1, H, W].\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor [B, 1, H, W].\n",
    "        \"\"\"\n",
    "        # Add conditioning channel if provided.\n",
    "        if condition is not None:\n",
    "            condition = condition.expand(-1, 1, x.shape[2], x.shape[3])\n",
    "            x = torch.cat([x, condition], dim=1)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = self.inc(x, t_emb)\n",
    "        x2 = self.down1[0](x1)  # MaxPool2d\n",
    "        x2 = self.down1[1](x2, t_emb)\n",
    "        x2 = self.down1[2](x2)\n",
    "        \n",
    "        x3 = self.down2[0](x2)\n",
    "        x3 = self.down2[1](x3, t_emb)\n",
    "        x3 = self.down2[2](x3)\n",
    "        \n",
    "        x4 = self.down3[0](x3)\n",
    "        x4 = self.down3[1](x4, t_emb)\n",
    "        x4 = self.down3[2](x4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x4 = self.bot1(x4, t_emb)\n",
    "        x4 = self.bot_attn(x4)\n",
    "        if context is not None:\n",
    "            # Project context to match bottleneck dimensions\n",
    "            context_proj = self.context_proj(context)\n",
    "            x4 = self.cross_attn(x4, context_proj)\n",
    "        x4 = self.bot2(x4, t_emb)\n",
    "        x4 = self.bot3(x4, t_emb)\n",
    "        \n",
    "        # Decoder\n",
    "        # Upsample x4 to match spatial dims of x3\n",
    "        x4 = F.interpolate(x4, size=x3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x4, x3], dim=1)\n",
    "        x = self.up1[0](x)\n",
    "        x = self.up1[1](x, t_emb)\n",
    "        x = self.up1[2](x)\n",
    "        \n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up2[0](x)\n",
    "        x = self.up2[1](x, t_emb)\n",
    "        x = self.up2[2](x)\n",
    "        \n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up3[0](x, t_emb)\n",
    "        \n",
    "        return self.outc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMTrainer:\n",
    "    \"\"\"Denoising Diffusion Probabilistic Models (DDPM) Trainer.\n",
    "    Handles the training process, including:\n",
    "    - Forward/reverse diffusion processes\n",
    "    - Optimization\n",
    "    - Mixed precision training\n",
    "    - Sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, model, n_timesteps=1000, beta_start=1e-4, beta_end=0.02,\n",
    "        lr=1e-4, device=\"cuda\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: UNet model instance\n",
    "            n_timesteps (int): Number of diffusion timesteps\n",
    "            beta_start (float): Starting noise schedule value\n",
    "            beta_end (float): Ending noise schedule value\n",
    "            lr (float): Learning rate for Adam optimizer\n",
    "            device (str): Device to run on (\"cuda\" or \"cpu\")\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Pre-compute values for diffusion process\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def diffuse_step(self, x_0, t):\n",
    "        \"\"\"Forward diffusion step: adds noise to image according to timestep.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Original clean image\n",
    "            t (torch.Tensor): Timesteps for batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (noisy image, noise added)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)  # Random noise\n",
    "        \n",
    "        # Get noise scaling factors for timestep t\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply forward diffusion equation\n",
    "        x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def train_one_batch(self, x_0, condition=None, context=None):\n",
    "        \"\"\"Trains model on a single batch.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Clean images [B, C, H, W]\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            float: Batch loss value\n",
    "        \"\"\"\n",
    "        batch_size = x_0.shape[0]\n",
    "        # Sample random timesteps for batch\n",
    "        t = torch.randint(0, self.n_timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Apply forward diffusion\n",
    "        x_t, noise = self.diffuse_step(x_0, t)\n",
    "\n",
    "        # Forward pass (removed autocast since it's not supported on MPS)\n",
    "        noise_pred = self.model(x_t, t, condition=condition, context=context)\n",
    "        # Calculate loss between predicted and actual noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, condition=None, context=None, shape=None, n_steps=None):\n",
    "        \"\"\"Generates samples using the reverse diffusion process.\n",
    "        \n",
    "        Args:\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            shape (tuple): Shape of samples to generate\n",
    "            n_steps (int, optional): Number of sampling steps\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Generated samples\n",
    "        \"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_timesteps\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x_t = torch.randn(shape, device=self.device)\n",
    "        \n",
    "        # Gradually denoise the sample\n",
    "        for t in reversed(range(n_steps)):\n",
    "            t_batch = torch.ones(shape[0], device=self.device, dtype=torch.long) * t\n",
    "            \n",
    "            # Predict noise in current sample\n",
    "            noise_pred = self.model(x_t, t_batch, condition=condition, context=context)\n",
    "            \n",
    "            # Get diffusion parameters for timestep t\n",
    "            alpha_t = self.alphas[t]\n",
    "            alpha_t_cumprod = self.alphas_cumprod[t]\n",
    "            beta_t = self.betas[t]\n",
    "            \n",
    "            # Add noise only if not the final step\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = 0.\n",
    "            \n",
    "            # Apply reverse diffusion equation\n",
    "            x_t = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - beta_t / torch.sqrt(1 - alpha_t_cumprod) * noise_pred\n",
    "            ) + torch.sqrt(beta_t) * noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Tracks and computes various metrics during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            device: Device to run metrics computation on\n",
    "        \"\"\"\n",
    "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
    "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets all metrics for new epoch.\"\"\"\n",
    "        self.train_losses = []\n",
    "        self.psnr_scores = []\n",
    "        self.ssim_scores = []\n",
    "    \n",
    "    def update(self, pred, target, loss=None):\n",
    "        \"\"\"Updates metrics with new batch results.\"\"\"\n",
    "        if loss is not None:\n",
    "            self.train_losses.append(loss)\n",
    "        if pred is not None and target is not None:\n",
    "            self.psnr_scores.append(self.psnr(pred, target).item())\n",
    "            self.ssim_scores.append(self.ssim(pred, target).item())\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Returns average metrics for the current period.\"\"\"\n",
    "        return {\n",
    "            'loss': np.mean(self.train_losses) if self.train_losses else 0,\n",
    "            'psnr': np.mean(self.psnr_scores) if self.psnr_scores else 0,\n",
    "            'ssim': np.mean(self.ssim_scores) if self.ssim_scores else 0\n",
    "        }\n",
    "\n",
    "def visualize_samples(frontal_real, lateral_real, lateral_gen, epoch, step, loss_H, PSNR, SSIM, save=True):\n",
    "    \"\"\"Creates visualization grid of real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        frontal_real, lateral_real: Real frontal images and lateral images\n",
    "        lateral_gen: Generated lateral images\n",
    "        epoch (int): Current epoch\n",
    "        step (int): Current step\n",
    "        save (bool): Whether to save the plot\n",
    "        loss_H (list): Training loss history\n",
    "        PSNR (float): Peak Signal-to-Noise Ratio of generated image\n",
    "        SSIM (float): Structural Similarity Index of generated image\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 6))\n",
    "    \n",
    "    # Real T1\n",
    "    axes[0].imshow(frontal_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[0].set_title('Frontal Image (Real)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Generated T2 from T1\n",
    "    axes[2].imshow(lateral_gen[0, 0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[2].set_title(f'Sampled Lateral Image\\nPSNR: {PSNR:.4f}, SSIM: {SSIM:.4f}')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # Real T2\n",
    "    axes[1].imshow(lateral_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[1].set_title('Lateral Image (Real)')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Loss Curve\n",
    "    axes[3].plot(loss_H)\n",
    "    axes[3].set_title('Training Loss (MSE)')\n",
    "    axes[3].set_xlabel('Steps')\n",
    "    axes[3].set_ylabel('Loss')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'E:/NEU_CSYE_7374/Final_Project/Visualizations/Diffusion_WithAttention/WithAttention_samples_epoch{epoch}_step{step}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3818 Frontal images and 3648 Lateral images\n",
      "Found 3510 paired Frontal/Lateral datasets with descriptions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193a05f564b4cbdb6522e101e4bb267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "Average Loss: 0.1877\n",
      "Average PSNR: 4.41\n",
      "Average SSIM: 0.0883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26138bc0bede4f158966956986536bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Average Loss: 0.0433\n",
      "Average PSNR: 8.64\n",
      "Average SSIM: 0.1640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24edaec3b3642658287ee3200eb4d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Average Loss: 0.0325\n",
      "Average PSNR: 8.20\n",
      "Average SSIM: 0.1499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e89fed837f4366a2e11395899ece0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class containing all training parameters and paths.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.frontal_dir = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"Classified_Images\" / \"Frontal\"\n",
    "        self.lateral_dir = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"Classified_Images\" / \"Lateral\"\n",
    "        self.excel_file = parent_dir / \"Data\" / \"chest-xrays-indiana-university\" / \"image_descriptions.csv\"\n",
    "        \n",
    "        # Model parameters\n",
    "        self.in_channels = 2  # Image + condition channel\n",
    "        self.time_channels = 256\n",
    "        self.n_channels = 64\n",
    "        self.n_timesteps = 300\n",
    "        self.beta_start = 1e-4\n",
    "        self.beta_end = 0.02\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 1\n",
    "    \n",
    "        self.num_epochs = 100\n",
    "        self.lr = 1e-4\n",
    "        self.save_interval = 100  # Save checkpoints every N steps\n",
    "        \n",
    "        # Directories setup - go up one level from notebooks to root\n",
    "        self.checkpoint_dir = Path(\"E:/NEU_CSYE_7374/Final_Project/Checkpoints/Diffusion_WithAttention\")\n",
    "        self.vis_dir = Path(\"E:/NEU_CSYE_7374/Final_Project/Visualizations/Diffusion_WithAttention\") \n",
    "        self.log_dir = Path(\"E:/NEU_CSYE_7374/Final_Project/Logs/Diffusion_WithAttention\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Cell 8: Training Functions\n",
    "\n",
    "def load_checkpoint(trainer, checkpoint_path):\n",
    "    \"\"\"Loads model and optimizer state from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (epoch, global_step)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['global_step']\n",
    "\n",
    "def save_checkpoint(trainer, epoch, global_step):\n",
    "    \"\"\"Saves model and optimizer state to checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        epoch: Current epoch\n",
    "        global_step: Current global step\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': trainer.model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "    }\n",
    "    torch.save(checkpoint, \n",
    "               config.checkpoint_dir / f'model_epoch{epoch}_step{global_step}.pt')\n",
    "\n",
    "# Main Training Loop\n",
    "\n",
    "def train_diffusion():\n",
    "    \"\"\"Main training function that handles the complete training pipeline.\"\"\"\n",
    "    \n",
    "    # Initialize tensorboard writer\n",
    "    writer = SummaryWriter(config.log_dir)\n",
    "    \n",
    "    # Initialize dataset and dataloader\n",
    "    # dataset = ChestXrayDataset(\n",
    "    #     frontal_dir=frontal_dir,\n",
    "    #     lateral_dir=lateral_dir,\n",
    "    #     excel_file=excel_file_3,\n",
    "    #     paired=True\n",
    "    # )\n",
    "\n",
    "\n",
    "    Full_dataset = ChestXrayDataset(\n",
    "        frontal_dir=config.frontal_dir,\n",
    "        lateral_dir=config.lateral_dir,\n",
    "        excel_file=config.excel_file,\n",
    "        paired=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    #\n",
    "    Train_size = 1000\n",
    "    Train_dataset = Subset(Full_dataset, range(Train_size))\n",
    "    Val_dataset = Subset(Full_dataset, range(Train_size, 1200))\n",
    "    \n",
    "    Train_dataloader = DataLoader(\n",
    "        Train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    Val_dataloader = DataLoader(\n",
    "        Val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    # model = UNet(\n",
    "    #     in_channels=config.in_channels,\n",
    "    #     time_channels=config.time_channels,\n",
    "    #     n_channels=config.n_channels,\n",
    "    #     use_self_attention=False,\n",
    "    #     use_cross_attention=False,\n",
    "    # )\n",
    "\n",
    "    model = UNet(\n",
    "        in_channels=config.in_channels,\n",
    "        time_channels=config.time_channels,\n",
    "        n_channels=config.n_channels,\n",
    "        use_self_attention=True,\n",
    "        use_cross_attention=True,\n",
    "    )\n",
    "    \n",
    "    trainer = DDPMTrainer(\n",
    "        model=model,\n",
    "        n_timesteps=config.n_timesteps,\n",
    "        beta_start=config.beta_start,\n",
    "        beta_end=config.beta_end,\n",
    "        lr=config.lr,\n",
    "        device=config.device\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics tracker\n",
    "    metrics = MetricsTracker(config.device)\n",
    "    loss_H = []\n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    if config.checkpoint_dir.exists():\n",
    "        checkpoints = list(config.checkpoint_dir.glob('model_epoch*_step*.pt'))\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "            # Load checkpoint with weights_only=True to avoid security warning\n",
    "            start_epoch, global_step = load_checkpoint(trainer, latest_checkpoint, weights_only=True)\n",
    "            print(f\"Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        metrics.reset()\n",
    "        epoch_pbar = tqdm(Train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(epoch_pbar):\n",
    "            # Move data to device\n",
    "            frontal = batch['Frontal'].to(config.device)\n",
    "            lateral = batch['Lateral'].to(config.device)\n",
    "            description = batch['Description'].to(config.device)\n",
    "            # Train Description -> lateral\n",
    "            loss_Description_Lateral = trainer.train_one_batch(\n",
    "                x_0=lateral,                # Target is lateral image\n",
    "                condition=description,      # Condition on Description\n",
    "                context=frontal             # Cross-attention sees Frontal image\n",
    "            )\n",
    "            \n",
    "            # # Train T2 -> T1\n",
    "            # loss_Description_Lateral = trainer.train_one_batch(\n",
    "            #     x_0=t1,            # Target is T1\n",
    "            #     condition=t2,      # Condition on T2\n",
    "            #     context=t2         # Cross-attention sees T2\n",
    "            # )\n",
    "            \n",
    "            # Update metrics\n",
    "            loss = loss_Description_Lateral\n",
    "            metrics.update(None, None, loss)\n",
    "            loss_H.append(loss)\n",
    "\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f\"{loss:.4f}\",\n",
    "                'step': global_step\n",
    "            })\n",
    "            \n",
    "            # Checkpoint and visualization\n",
    "            if global_step % config.save_interval == 0:\n",
    "                trainer.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Generate samples\n",
    "                    lateral_gen = trainer.sample(\n",
    "                        condition=description,\n",
    "                        context=frontal,\n",
    "                        shape=frontal.shape\n",
    "                    )\n",
    "\n",
    "                    # t1_gen = trainer.sample(\n",
    "                    #     condition=t2,\n",
    "                    #     context=t2,\n",
    "                    #     shape=t2.shape\n",
    "                    # )\n",
    "                    \n",
    "                    # Calculate metrics for generated images\n",
    "                    metrics.update(lateral_gen, lateral)\n",
    "                    # metrics.update(t1_gen, t1)\n",
    "                    \n",
    "\n",
    "                    # Log to tensorboard\n",
    "                    current_metrics = metrics.get_metrics()\n",
    "                    writer.add_scalar('Loss/train', current_metrics['loss'], global_step)\n",
    "                    writer.add_scalar('Metrics/PSNR', current_metrics['psnr'], global_step)\n",
    "                    writer.add_scalar('Metrics/SSIM', current_metrics['ssim'], global_step)\n",
    "                    writer.add_images('Samples/Frontal', frontal, global_step)\n",
    "                    writer.add_images('Samples/Lateral', lateral, global_step)\n",
    "                    # writer.add_images('Samples/T1_generated', t1_gen, global_step)\n",
    "                    writer.add_images('Samples/Lateral_generated', lateral_gen, global_step)\n",
    "                    \n",
    "                    \n",
    "                    # Visualize samples\n",
    "                    visualize_samples(\n",
    "                        frontal, lateral,\n",
    "                        lateral_gen,\n",
    "                        epoch, global_step,\n",
    "                        loss_H,\n",
    "                        current_metrics['psnr'],\n",
    "                        current_metrics['ssim']\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    save_checkpoint(trainer, epoch, global_step)\n",
    "                \n",
    "                trainer.model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        # End of epoch\n",
    "        epoch_metrics = metrics.get_metrics()\n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Average Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"Average PSNR: {epoch_metrics['psnr']:.2f}\")\n",
    "        print(f\"Average SSIM: {epoch_metrics['ssim']:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    train_diffusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEU_CSYE_7374_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
